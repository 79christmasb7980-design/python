# ë„¤ì´ë²„ ê¸ˆìœµ í¸ì…ì¢…ëª©ìƒìœ„ ë°ì´í„° í¬ë¡¤ë§ ê°€ì´ë“œ

ë„¤ì´ë²„ ê¸ˆìœµ í˜ì´ì§€ì—ì„œ **BeautifulSoup**ì„ ì‚¬ìš©í•˜ì—¬ í¸ì…ì¢…ëª©ìƒìœ„ ë°ì´í„°ë¥¼ í¬ë¡¤ë§í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤.

## ğŸ“Œ ê°œìš”

- **ëŒ€ìƒ URL**: `https://finance.naver.com/sise/sise_index.naver?code=KPI200`
- **ë°ì´í„°**: ì½”ìŠ¤í”¼200(KPI200)ì˜ ì§€ìˆ˜ì •ë³´ ë° ì¢…ëª© ë°ì´í„°
- **ë¼ì´ë¸ŒëŸ¬ë¦¬**: BeautifulSoup4, requests

---

## ğŸ”§ ì„¤ì¹˜ ë° ì¤€ë¹„

### 1. í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜

```bash
pip install requests beautifulsoup4
```

### 2. Python 3.6 ì´ìƒ í•„ìš”

```bash
python --version
```

---

## ğŸ“– ê¸°ë³¸ í¬ë¡¤ë§ ì½”ë“œ

### ê°„ë‹¨í•œ ë²„ì „ (15ì¤„)

```python
import requests
from bs4 import BeautifulSoup

# URLê³¼ í—¤ë” ì„¤ì •
url = "https://finance.naver.com/sise/sise_index.naver?code=KPI200"
headers = {'User-Agent': 'Mozilla/5.0'}

# í˜ì´ì§€ ìš”ì²­
response = requests.get(url, headers=headers)
response.encoding = 'utf-8'

# HTML íŒŒì‹±
soup = BeautifulSoup(response.content, 'html.parser')

# í…Œì´ë¸” ì°¾ê¸°
tables = soup.find_all('table')

# ê° í…Œì´ë¸”ì˜ ë°ì´í„° ì¶”ì¶œ
for idx, table in enumerate(tables):
    rows = table.find_all('tr')
    
    print(f"\n=== í…Œì´ë¸” #{idx} ===")
    
    # í—¤ë” ì¶œë ¥
    if rows:
        headers = [th.get_text(strip=True) for th in rows[0].find_all(['th', 'td'])]
        print(f"í—¤ë”: {headers}")
    
    # ë°ì´í„° í–‰ ì¶œë ¥
    print("ë°ì´í„°:")
    for row in rows[1:]:
        cells = [td.get_text(strip=True) for td in row.find_all('td')]
        if cells:
            print(cells)
```

---

## ğŸ“Š ì¤‘ê¸‰ ë²„ì „ (í´ë˜ìŠ¤ ê¸°ë°˜)

```python
import requests
from bs4 import BeautifulSoup
import csv


class NaverStockCrawler:
    """ë„¤ì´ë²„ ê¸ˆìœµ í¬ë¡¤ëŸ¬"""
    
    def __init__(self):
        self.base_url = "https://finance.naver.com/sise/sise_index.naver"
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
    
    def crawl(self, code):
        """ì§€ìˆ˜ ë°ì´í„° í¬ë¡¤ë§"""
        try:
            url = f"{self.base_url}?code={code}"
            response = requests.get(url, headers=self.headers, timeout=10)
            response.encoding = 'utf-8'
            
            soup = BeautifulSoup(response.content, 'html.parser')
            tables = soup.find_all('table')
            
            results = []
            for table in tables:
                table_data = {
                    'headers': [],
                    'rows': []
                }
                
                rows = table.find_all('tr')
                
                # í—¤ë”
                if rows:
                    table_data['headers'] = [
                        th.get_text(strip=True) 
                        for th in rows[0].find_all(['th', 'td'])
                    ]
                
                # ë°ì´í„°
                for row in rows[1:]:
                    cells = row.find_all('td')
                    if cells:
                        row_data = [cell.get_text(strip=True) for cell in cells]
                        if any(row_data):
                            table_data['rows'].append(row_data)
                
                results.append(table_data)
            
            return results
        
        except Exception as e:
            print(f"ì—ëŸ¬: {e}")
            return None
    
    def save_csv(self, data, filename):
        """CSVë¡œ ì €ì¥"""
        for idx, table in enumerate(data):
            csv_file = filename.replace('.csv', f'_table{idx}.csv')
            with open(csv_file, 'w', newline='', encoding='utf-8-sig') as f:
                writer = csv.writer(f)
                if table['headers']:
                    writer.writerow(table['headers'])
                writer.writerows(table['rows'])
            print(f"ì €ì¥: {csv_file}")


# ì‚¬ìš© ì˜ˆì œ
if __name__ == "__main__":
    crawler = NaverStockCrawler()
    
    # í¬ë¡¤ë§
    data = crawler.crawl("KPI200")
    
    # ì¶œë ¥
    for table in data:
        print(f"í—¤ë”: {table['headers']}")
        for row in table['rows'][:5]:
            print(row)
        print()
    
    # CSV ì €ì¥
    crawler.save_csv(data, "kospi200.csv")
```

---

## ğŸ¯ ì£¼ìš” ì‚¬ìš© ë°©ë²•

### 1. íŠ¹ì • ì§€ìˆ˜ ì½”ë“œë¡œ í¬ë¡¤ë§

```python
# ì½”ìŠ¤í”¼200
data = crawler.crawl("KPI200")

# ì½”ìŠ¤í”¼
data = crawler.crawl("KOSPI")

# ì½”ìŠ¤ë‹¥
data = crawler.crawl("KOSDAQ")

# ì½”ìŠ¤í”¼100
data = crawler.crawl("KOSPI100")
```

### 2. ë°ì´í„° í•„í„°ë§

```python
# íŠ¹ì • ì¢…ëª© ì°¾ê¸°
for row in data[1]['rows']:  # ë‘ ë²ˆì§¸ í…Œì´ë¸”ì˜ ì¢…ëª© ë°ì´í„°
    if 'ì‚¼ì„±' in row[1]:  # 2ë²ˆì§¸ ì—´ì—ì„œ ì‚¼ì„± ê²€ìƒ‰
        print(row)
```

### 3. ë°ì´í„° ë¶„ì„

```python
# ê°€ê²© ê¸°ì¤€ ì •ë ¬
def get_price(row):
    try:
        return int(row[2].replace(',', ''))
    except:
        return 0

data[1]['rows'].sort(key=get_price, reverse=True)
```

---

## âš ï¸ ì£¼ì˜ì‚¬í•­

### 1. User-Agent í•„ìˆ˜
ë„¤ì´ë²„ëŠ” User-Agent ê²€ì¦ì„ í•˜ë¯€ë¡œ ë°˜ë“œì‹œ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.

```python
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
}
```

### 2. íƒ€ì„ì•„ì›ƒ ì„¤ì •
ë„¤íŠ¸ì›Œí¬ ì§€ì—°ì— ëŒ€ë¹„í•˜ì—¬ íƒ€ì„ì•„ì›ƒì„ ì„¤ì •í•©ë‹ˆë‹¤.

```python
response = requests.get(url, headers=headers, timeout=10)
```

### 3. ì¸ì½”ë”© ì„¤ì •
í•œê¸€ í…ìŠ¤íŠ¸ë¥¼ ì˜¬ë°”ë¥´ê²Œ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ UTF-8 ì¸ì½”ë”©ì„ ì„¤ì •í•©ë‹ˆë‹¤.

```python
response.encoding = 'utf-8'
```

### 4. ë„ˆë¬´ ë§ì€ ìš”ì²­ ê¸ˆì§€
ì§§ì€ ì‹œê°„ì— ì—¬ëŸ¬ ë²ˆ ìš”ì²­í•˜ë©´ ì°¨ë‹¨ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
import time
time.sleep(1)  # 1ì´ˆ ëŒ€ê¸°
```

---

## ğŸ”„ Seleniumì„ ì‚¬ìš©í•œ ë™ì  í˜ì´ì§€ í¬ë¡¤ë§

JavaScriptë¡œ ë Œë”ë§ë˜ëŠ” ì½˜í…ì¸ ê°€ í•„ìš”í•œ ê²½ìš° Seleniumì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

### ì„¤ì¹˜

```bash
pip install selenium
```

### ì½”ë“œ ì˜ˆì œ

```python
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from bs4 import BeautifulSoup
import time

# Chrome ë“œë¼ì´ë²„ ì„¤ì •
options = webdriver.ChromeOptions()
options.add_argument('--headless')  # ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰

driver = webdriver.Chrome(options=options)

try:
    # í˜ì´ì§€ ë¡œë“œ
    url = "https://finance.naver.com/sise/sise_index.naver?code=KPI200"
    driver.get(url)
    
    # JavaScript ë¡œë”© ëŒ€ê¸°
    time.sleep(3)
    
    # BeautifulSoupìœ¼ë¡œ íŒŒì‹±
    soup = BeautifulSoup(driver.page_source, 'html.parser')
    
    # í…Œì´ë¸” ì¶”ì¶œ
    tables = soup.find_all('table')
    # ... ì´í›„ ì²˜ë¦¬
finally:
    driver.quit()
```

---

## ğŸ“ˆ ì‹¤í–‰ ê²°ê³¼ ì˜ˆì‹œ

```
==========================================================================================
ğŸ“Š KPI200 - 2025-11-13T10:10:28.148211
==========================================================================================

ğŸ“‹ í…Œì´ë¸” #0
    ì»¬ëŸ¼: ì½”ìŠ¤í”¼200 | 587.88 |  | ìƒí•œì¢…ëª©ìˆ˜ | 0
    ë°ì´í„° (6í–‰):
       1. 1.15 |  | 78
       2. +0.20% |  | 0
       3. 588.28 |  | 116
       4. 582.82 |  | 5

ğŸ“‹ í…Œì´ë¸” #1
    ì»¬ëŸ¼:  | ì‚¼ì„±ì „ì | 103,700 |
    ë°ì´í„° (4í–‰):
       1.  | SKí•˜ì´ë‹‰ìŠ¤ | 615,000 |
       2.  | ì—ì´ë¹„ì—˜ë°”ì´ì˜¤ | 164,000 |
       3.  | ë‘ì‚°ì—ë„ˆë¹Œë¦¬í‹° | 80,400 |
       4.  | í•œí™”ì˜¤ì…˜ | 124,900 |

âœ… JSON ì €ì¥: stock_data_KPI200.json
âœ… CSV ì €ì¥: stock_data_KPI200_table0.csv
```

---

## ğŸ› ï¸ ë¬¸ì œ í•´ê²°

### Q1: "ModuleNotFoundError: No module named 'requests'"

**í•´ê²°ë°©ë²•:**
```bash
pip install requests beautifulsoup4
```

### Q2: í•œê¸€ ê¹¨ì§ ë¬¸ì œ

**í•´ê²°ë°©ë²•:**
```python
response.encoding = 'utf-8'
```

### Q3: ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•ŠìŒ (JavaScript ë Œë”ë§)

**í•´ê²°ë°©ë²•:**
Selenium ì‚¬ìš©ìœ¼ë¡œ ë³€ê²½

```python
from selenium import webdriver
# ìœ„ì˜ Selenium ì˜ˆì œ ì°¸ì¡°
```

### Q4: ìš”ì²­ ê±°ë¶€ (403 Forbidden)

**í•´ê²°ë°©ë²•:**
í—¤ë”ì— Referer ì¶”ê°€

```python
headers = {
    'User-Agent': 'Mozilla/5.0',
    'Referer': 'https://finance.naver.com/'
}
```

---

## ğŸ“š ì°¸ê³  ìë£Œ

- [BeautifulSoup ê³µì‹ ë¬¸ì„œ](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)
- [requests ê³µì‹ ë¬¸ì„œ](https://docs.python-requests.org/)
- [Selenium ê³µì‹ ë¬¸ì„œ](https://selenium-python.readthedocs.io/)

---

## âœ… ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] Python 3.6 ì´ìƒ ì„¤ì¹˜ í™•ì¸
- [ ] `requests`, `beautifulsoup4` ì„¤ì¹˜ í™•ì¸
- [ ] ê¸°ë³¸ í¬ë¡¤ë§ ì½”ë“œ ì‹¤í–‰ ì„±ê³µ
- [ ] CSV íŒŒì¼ë¡œ ì €ì¥ ì„±ê³µ
- [ ] ë°ì´í„° ë¶„ì„/í•„í„°ë§ ì™„ë£Œ

---

**ì‘ì„±ì¼**: 2025ë…„ 11ì›” 13ì¼  
**ë§ˆì§€ë§‰ ìˆ˜ì •**: 2025ë…„ 11ì›” 13ì¼
