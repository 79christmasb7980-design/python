"""
ë„¤ì´ë²„ ê¸ˆìœµ - ì½”ìŠ¤í”¼200 í¸ì…ì¢…ëª©ìƒìœ„ ë°ì´í„° í¬ë¡¤ë§
URL: https://finance.naver.com/sise/sise_index.naver?code=KPI200

ì‘ì„±ì¼: 2025ë…„ 11ì›”
ì„¤ëª…: BeautifulSoupê³¼ Seleniumì„ í™œìš©í•œ ì›¹ í¬ë¡¤ë§ ì˜ˆì œ
"""

import requests
from bs4 import BeautifulSoup
import json
from typing import List, Dict
import time


class NaverFinanceCrawler:
    """ë„¤ì´ë²„ ê¸ˆìœµ í˜ì´ì§€ í¬ë¡¤ëŸ¬"""
    
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        self.base_url = "https://finance.naver.com/sise/sise_index.naver"
    
    def crawl_with_beautifulsoup(self, code: str = "KPI200") -> List[Dict]:
        """
        BeautifulSoupì„ ì‚¬ìš©í•˜ì—¬ ì½”ìŠ¤í”¼200 í˜ì´ì§€ í¬ë¡¤ë§
        
        Args:
            code: ì§€ìˆ˜ ì½”ë“œ (ê¸°ë³¸ê°’: KPI200 = ì½”ìŠ¤í”¼200)
        
        Returns:
            í…Œì´ë¸” ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        """
        try:
            print(f"ğŸ“¡ BeautifulSoupìœ¼ë¡œ {code} í˜ì´ì§€ ìš”ì²­ ì¤‘...\n")
            
            response = requests.get(
                f"{self.base_url}?code={code}",
                headers=self.headers,
                timeout=10
            )
            response.encoding = 'utf-8'
            
            if response.status_code != 200:
                print(f"âœ— ìš”ì²­ ì‹¤íŒ¨: {response.status_code}")
                return []
            
            print("âœ“ í˜ì´ì§€ ë¡œë“œ ì„±ê³µ\n")
            
            # HTML íŒŒì‹±
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # í…Œì´ë¸” ì¶”ì¶œ
            tables = soup.find_all('table')
            print(f"ë°œê²¬ëœ í…Œì´ë¸”: {len(tables)}ê°œ\n")
            
            all_data = []
            
            for idx, table in enumerate(tables):
                rows = table.find_all('tr')
                
                if len(rows) < 2:
                    continue
                
                # í—¤ë” ì¶”ì¶œ
                header_row = rows[0]
                headers_list = [
                    cell.get_text(strip=True) 
                    for cell in header_row.find_all(['th', 'td'])
                ]
                
                print(f"â”â”â” í…Œì´ë¸” #{idx} â”â”â”")
                print(f"í—¤ë”: {headers_list}")
                print("ë°ì´í„°:")
                
                table_data = {
                    'table_index': idx,
                    'headers': headers_list,
                    'rows': []
                }
                
                # ë°ì´í„° í–‰ ì¶”ì¶œ
                for row_idx, row in enumerate(rows[1:], 1):
                    cols = row.find_all('td')
                    if cols:
                        row_data = []
                        for col in cols:
                            # ë§í¬ê°€ ìˆìœ¼ë©´ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
                            link = col.find('a')
                            text = link.get_text(strip=True) if link else col.get_text(strip=True)
                            row_data.append(text)
                        
                        if any(row_data):  # ë¹ˆ í–‰ ì œì™¸
                            table_data['rows'].append(row_data)
                            print(f"  {row_idx}: {row_data}")
                
                print()
                all_data.append(table_data)
            
            return all_data
        
        except requests.exceptions.RequestException as e:
            print(f"âœ— ìš”ì²­ ì—ëŸ¬: {e}")
            return []
        except Exception as e:
            print(f"âœ— ì˜ˆê¸°ì¹˜ ì•Šì€ ì—ëŸ¬: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    def crawl_with_selenium(self, code: str = "KPI200") -> List[Dict]:
        """
        Seleniumì„ ì‚¬ìš©í•˜ì—¬ JavaScript ë™ì  ë¡œë”© í¬í•¨ í¬ë¡¤ë§
        (ì„¤ì¹˜ í•„ìš”: pip install selenium)
        
        Args:
            code: ì§€ìˆ˜ ì½”ë“œ
        
        Returns:
            í…Œì´ë¸” ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        """
        try:
            from selenium import webdriver
            from selenium.webdriver.common.by import By
            from selenium.webdriver.support.ui import WebDriverWait
            from selenium.webdriver.support import expected_conditions as EC
            
            print(f"ğŸ“¡ Seleniumìœ¼ë¡œ {code} í˜ì´ì§€ ìš”ì²­ ì¤‘...\n")
            
            # Chrome ì˜µì…˜ ì„¤ì •
            options = webdriver.ChromeOptions()
            options.add_argument('--headless')  # ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰
            options.add_argument('--no-sandbox')
            options.add_argument('--disable-dev-shm-usage')
            options.add_argument('--disable-gpu')
            
            driver = webdriver.Chrome(options=options)
            
            try:
                # í˜ì´ì§€ ë¡œë“œ
                url = f"{self.base_url}?code={code}"
                print(f"ğŸ”— URL: {url}\n")
                
                driver.get(url)
                
                # JavaScript ë¡œë”© ëŒ€ê¸° (ìµœëŒ€ 10ì´ˆ)
                print("â³ JavaScript ë¡œë”© ëŒ€ê¸° ì¤‘...")
                WebDriverWait(driver, 10).until(
                    lambda driver: driver.execute_script(
                        'return document.readyState'
                    ) == 'complete'
                )
                time.sleep(2)  # ì¶”ê°€ ë¡œë”© ì‹œê°„
                
                print("âœ“ í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ\n")
                
                # í˜ì´ì§€ ì†ŒìŠ¤ ì¶”ì¶œ
                soup = BeautifulSoup(driver.page_source, 'html.parser')
                
                # í…Œì´ë¸” ì¶”ì¶œ
                tables = soup.find_all('table')
                print(f"ë°œê²¬ëœ í…Œì´ë¸”: {len(tables)}ê°œ\n")
                
                all_data = []
                
                for idx, table in enumerate(tables):
                    rows = table.find_all('tr')
                    
                    if len(rows) < 2:
                        continue
                    
                    # í—¤ë” ì¶”ì¶œ
                    header_row = rows[0]
                    headers_list = [
                        cell.get_text(strip=True)
                        for cell in header_row.find_all(['th', 'td'])
                    ]
                    
                    print(f"â”â”â” í…Œì´ë¸” #{idx} â”â”â”")
                    print(f"í—¤ë”: {headers_list}")
                    print("ë°ì´í„°:")
                    
                    table_data = {
                        'table_index': idx,
                        'headers': headers_list,
                        'rows': []
                    }
                    
                    # ë°ì´í„° í–‰ ì¶”ì¶œ
                    for row_idx, row in enumerate(rows[1:], 1):
                        cols = row.find_all('td')
                        if cols:
                            row_data = [col.get_text(strip=True) for col in cols]
                            
                            if any(row_data):
                                table_data['rows'].append(row_data)
                                print(f"  {row_idx}: {row_data}")
                    
                    print()
                    all_data.append(table_data)
                
                return all_data
            
            finally:
                driver.quit()
        
        except ImportError:
            print("âš ï¸  Seleniumì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            print("ì„¤ì¹˜ ëª…ë ¹ì–´: pip install selenium")
            print("ë˜í•œ ChromeDriverë¥¼ ë‹¤ìš´ë¡œë“œí•´ì•¼ í•©ë‹ˆë‹¤.")
            return []
        except Exception as e:
            print(f"âœ— Selenium ì—ëŸ¬: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    def save_to_json(self, data: List[Dict], filename: str = "kospi200_data.json"):
        """
        ìˆ˜ì§‘í•œ ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
        
        Args:
            data: ì €ì¥í•  ë°ì´í„°
            filename: ì €ì¥ íŒŒì¼ëª…
        """
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            print(f"âœ“ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {filename}")
        except Exception as e:
            print(f"âœ— ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def save_to_csv(self, data: List[Dict], filename: str = "kospi200_data.csv"):
        """
        ìˆ˜ì§‘í•œ ë°ì´í„°ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥
        
        Args:
            data: ì €ì¥í•  ë°ì´í„°
            filename: ì €ì¥ íŒŒì¼ëª…
        """
        try:
            import csv
            
            for table_idx, table in enumerate(data):
                csv_filename = filename.replace('.csv', f'_table{table_idx}.csv')
                
                with open(csv_filename, 'w', newline='', encoding='utf-8-sig') as f:
                    writer = csv.writer(f)
                    # í—¤ë” ì‘ì„±
                    writer.writerow(table['headers'])
                    # ë°ì´í„° ì‘ì„±
                    writer.writerows(table['rows'])
                
                print(f"âœ“ CSV ì €ì¥ ì™„ë£Œ: {csv_filename}")
        except Exception as e:
            print(f"âœ— CSV ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def print_results(self, data: List[Dict], method: str = "BeautifulSoup"):
        """
        í¬ë¡¤ë§ ê²°ê³¼ ì¶œë ¥
        
        Args:
            data: í¬ë¡¤ë§ ë°ì´í„°
            method: í¬ë¡¤ë§ ë°©ë²•
        """
        if not data:
            print("âŒ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print(f"\n{'='*80}")
        print(f"í¬ë¡¤ë§ ê²°ê³¼ (ë°©ë²•: {method})")
        print(f"{'='*80}\n")
        
        for table in data:
            idx = table['table_index']
            headers = table['headers']
            rows = table['rows']
            
            print(f"ğŸ“Š í…Œì´ë¸” #{idx}")
            print(f"í–‰ ê°œìˆ˜: {len(rows)}")
            print(f"ì»¬ëŸ¼: {', '.join(headers)}")
            print("-" * 100)
            
            for row in rows:
                print(" | ".join(row))
            
            print("\n")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    
    crawler = NaverFinanceCrawler()
    
    print("="*80)
    print("ë„¤ì´ë²„ ê¸ˆìœµ - ì½”ìŠ¤í”¼200(KPI200) ë°ì´í„° í¬ë¡¤ë§")
    print("="*80 + "\n")
    
    # ë°©ë²• 1: BeautifulSoup ì‚¬ìš© (í•­ìƒ ì‹¤í–‰ ê°€ëŠ¥)
    print("\n[ë°©ë²• 1] BeautifulSoup ì‚¬ìš©")
    print("-" * 80)
    results_bs = crawler.crawl_with_beautifulsoup("KPI200")
    crawler.print_results(results_bs, "BeautifulSoup")
    
    # ê²°ê³¼ ì €ì¥
    if results_bs:
        crawler.save_to_json(results_bs, "kospi200_beautifulsoup.json")
        crawler.save_to_csv(results_bs, "kospi200_beautifulsoup.csv")
    
    # ë°©ë²• 2: Selenium ì‚¬ìš© (ì„ íƒì‚¬í•­)
    # ì£¼ì„ ì œê±°í•˜ê³  Selenium ì„¤ì¹˜ í›„ ì‚¬ìš© ê°€ëŠ¥
    """
    print("\n[ë°©ë²• 2] Selenium ì‚¬ìš©")
    print("-" * 80)
    results_selenium = crawler.crawl_with_selenium("KPI200")
    crawler.print_results(results_selenium, "Selenium")
    
    if results_selenium:
        crawler.save_to_json(results_selenium, "kospi200_selenium.json")
        crawler.save_to_csv(results_selenium, "kospi200_selenium.csv")
    """
    
    print("\nâœ“ í¬ë¡¤ë§ ì‘ì—… ì™„ë£Œ!")


if __name__ == "__main__":
    main()
