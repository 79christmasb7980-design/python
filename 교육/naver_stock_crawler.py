"""
ë„¤ì´ë²„ ê¸ˆìœµ í¸ì…ì¢…ëª©ìƒìœ„ ë°ì´í„° í¬ë¡¤ë§
======================================

ì´ ì½”ë“œëŠ” BeautifulSoupì„ ì‚¬ìš©í•˜ì—¬ ë„¤ì´ë²„ ê¸ˆìœµì—ì„œ ë‹¤ì–‘í•œ ì§€ìˆ˜ ì •ë³´ë¥¼ 
í¬ë¡¤ë§í•˜ëŠ” ì˜ˆì œì…ë‹ˆë‹¤.

ğŸ“Œ í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬:
   - requests
   - beautifulsoup4

ğŸ’¾ ì„¤ì¹˜ ë°©ë²•:
   pip install requests beautifulsoup4

ğŸŒ ì‚¬ìš© ê°€ëŠ¥í•œ ì§€ìˆ˜ ì½”ë“œ:
   - KPI200: ì½”ìŠ¤í”¼200
   - KOSPI: ì½”ìŠ¤í”¼
   - KOSDAQ: ì½”ìŠ¤ë‹¥
   - KOSPI100: ì½”ìŠ¤í”¼100
"""

import requests
from bs4 import BeautifulSoup
import json
from datetime import datetime


class NaverStockCrawler:
    """ë„¤ì´ë²„ ê¸ˆìœµ í¬ë¡¤ëŸ¬ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.base_url = "https://finance.naver.com/sise/sise_index.naver"
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
    
    def fetch_html(self, code):
        """HTML í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°"""
        try:
            response = requests.get(
                f"{self.base_url}?code={code}",
                headers=self.headers,
                timeout=10
            )
            response.encoding = 'utf-8'
            
            if response.status_code == 200:
                return response.content
            else:
                raise Exception(f"HTTP {response.status_code}")
        
        except Exception as e:
            print(f"âŒ ìš”ì²­ ì‹¤íŒ¨: {e}")
            return None
    
    def parse_tables(self, html_content):
        """HTMLì—ì„œ í…Œì´ë¸” ì¶”ì¶œ"""
        soup = BeautifulSoup(html_content, 'html.parser')
        tables = soup.find_all('table')
        
        parsed_data = []
        
        for table_idx, table in enumerate(tables):
            rows = table.find_all('tr')
            
            if not rows:
                continue
            
            # í—¤ë” ì¶”ì¶œ
            header_row = rows[0]
            headers = [
                th.get_text(strip=True)
                for th in header_row.find_all(['th', 'td'])
            ]
            
            # ë°ì´í„° í–‰ ì¶”ì¶œ
            data_rows = []
            for row in rows[1:]:
                cells = row.find_all('td')
                if cells:
                    row_data = [
                        cell.get_text(strip=True)
                        for cell in cells
                    ]
                    if any(row_data):  # ë¹ˆ í–‰ ì œì™¸
                        data_rows.append(row_data)
            
            # í…Œì´ë¸” ì •ë³´ ì €ì¥
            if headers or data_rows:
                parsed_data.append({
                    'table_index': table_idx,
                    'headers': headers,
                    'data': data_rows
                })
        
        return parsed_data
    
    def crawl(self, code):
        """í¬ë¡¤ë§ ì‹¤í–‰"""
        print(f"ğŸ” {code} í¬ë¡¤ë§ ì¤‘...")
        
        html = self.fetch_html(code)
        if html is None:
            return None
        
        parsed = self.parse_tables(html)
        
        return {
            'code': code,
            'timestamp': datetime.now().isoformat(),
            'tables': parsed
        }
    
    def print_result(self, result):
        """ê²°ê³¼ ì¶œë ¥"""
        if not result:
            print("âŒ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.\n")
            return
        
        print("\n" + "="*90)
        print(f"ğŸ“Š {result['code']} - {result['timestamp']}")
        print("="*90 + "\n")
        
        for table in result['tables']:
            print(f"ğŸ“‹ í…Œì´ë¸” #{table['table_index']}")
            
            # í—¤ë” ì¶œë ¥
            if table['headers']:
                print(f"    ì»¬ëŸ¼: {' | '.join(table['headers'])}")
            
            # ë°ì´í„° ì¶œë ¥
            print(f"    ë°ì´í„° ({len(table['data'])}í–‰):")
            for i, row in enumerate(table['data'][:10], 1):  # ì²˜ìŒ 10í–‰ë§Œ í‘œì‹œ
                print(f"      {i:2d}. {' | '.join(row)}")
            
            if len(table['data']) > 10:
                print(f"      ... ì™¸ {len(table['data']) - 10}í–‰")
            
            print()
    
    def export_json(self, result, filename):
        """JSONìœ¼ë¡œ ë‚´ë³´ë‚´ê¸°"""
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2)
            print(f"âœ… JSON ì €ì¥: {filename}")
        except Exception as e:
            print(f"âŒ JSON ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def export_csv(self, result, filename):
        """CSVë¡œ ë‚´ë³´ë‚´ê¸°"""
        import csv
        
        try:
            for table in result['tables']:
                csv_file = filename.replace(
                    '.csv',
                    f"_table{table['table_index']}.csv"
                )
                
                with open(csv_file, 'w', newline='', encoding='utf-8-sig') as f:
                    writer = csv.writer(f)
                    
                    # í—¤ë”
                    if table['headers']:
                        writer.writerow(table['headers'])
                    
                    # ë°ì´í„°
                    writer.writerows(table['data'])
                
                print(f"âœ… CSV ì €ì¥: {csv_file}")
        
        except Exception as e:
            print(f"âŒ CSV ì €ì¥ ì‹¤íŒ¨: {e}")


# ============================================================
# ì‚¬ìš© ì˜ˆì œ
# ============================================================

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    
    # í¬ë¡¤ëŸ¬ ìƒì„±
    crawler = NaverStockCrawler()
    
    # í¬ë¡¤ë§í•  ì§€ìˆ˜ ì½”ë“œ ëª©ë¡
    codes = ["KPI200", "KOSPI", "KOSDAQ"]
    
    print("="*90)
    print("ğŸŒ ë„¤ì´ë²„ ê¸ˆìœµ ë°ì´í„° í¬ë¡¤ë§")
    print("="*90 + "\n")
    
    all_results = []
    
    # ê° ì§€ìˆ˜ í¬ë¡¤ë§
    for code in codes:
        result = crawler.crawl(code)
        
        if result:
            crawler.print_result(result)
            all_results.append(result)
            
            # ê°œë³„ ì €ì¥
            crawler.export_json(result, f"stock_data_{code}.json")
            crawler.export_csv(result, f"stock_data_{code}.csv")
    
    # ì „ì²´ ê²°ê³¼ ì €ì¥
    if all_results:
        with open("stock_data_all.json", 'w', encoding='utf-8') as f:
            json.dump(all_results, f, ensure_ascii=False, indent=2)
        print(f"âœ… ì „ì²´ ë°ì´í„° ì €ì¥: stock_data_all.json")
    
    print("\nâœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ!\n")


if __name__ == "__main__":
    main()
